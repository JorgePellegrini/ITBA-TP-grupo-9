# -*- coding: utf-8 -*-
"""TP3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kTBsXYPTeKBRWvZBpGw0A68R7Q6xJxJ0

Se importan las librerias
"""

import numpy as np
import matplotlib.pyplot as plt
import math
import random
import cv2
import numpy as np
import os
import shutil
from google.colab import files

"""# **Ejercicio 1**"""

#TABLA DE VERDAD función AND
X1=np.array([-1,-1,1,1])
X2=np.array([-1,1,-1,1])
Yd=np.array([-1,-1,-1,1])
Ydcero=np.array([0,0,0,1])
#Se inicializa los pesos
np.random.seed(2)
W = np.random.rand(3)

# Se define la tasa de aprendizaje
n=0.05
#cantidad de iteraciones
iter=25
# Se inicializa el vector que contiene las predicciones y el error
yo = np.zeros(len(X1))
E = np.zeros(iter+1)

#se calcula el error al inicio

for i in range(len(X1)):
  yo[i]=np.sign(X1[i]*W[0]+X2[i]*W[1]+W[2])
  if yo[i]==-1 :
    yo[i]=0

E[0]=sum((yo-Ydcero)**2)

for i in range(iter):
# El contador permite el orden aleatorio
  Contador=np.random.permutation(len(X1))
  for j in range(len(Contador)):
# Se calcula yo, si difiere de Yd se actualiza los pesos
    yo[Contador[j]]=np.sign(X1[Contador[j]]*W[0]+X2[Contador[j]]*W[1]+W[2])
    if abs(yo[Contador[j]]-Yd[Contador[j]]) > 0 :
      deltaW=n*(Yd[Contador[j]]-yo[Contador[j]])*np.array([X1[Contador[j]],X2[Contador[j]],1])
      W=W+deltaW
    if yo[Contador[j]]==-1:
     yo[Contador[j]]=0
# Se calcula el error
  E[i+1]=sum((yo-Ydcero)**2)
#----------------------------------------------------------------------------------------------------------------------------
print(W)
fig, ax = plt.subplots()
ax.scatter([1], [1],color = 'black')
ax.scatter([-1,-1,1], [-1,1,-1],color = 'red')
ax.plot([-2, 2], [-(W[2]/W[1])-(-2)*(W[0]/W[1]), -(W[2]/W[1])-(2)*(W[0]/W[1])],color = 'black')
ax.set_title('Recta discriminadora',fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
ax.set_xlabel("X1", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
ax.set_ylabel("X2", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
ax.set_xlim(-2, 2)
ax.set_ylim(-2, 2)
plt.show()


fig, ax = plt.subplots()
ax.plot(E,color = 'black')
ax.set_xlabel("iteraciones", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
ax.set_ylabel("Error", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
plt.show()

"""Se repite el procedimiento para guardar la animación del ajuste"""

#TABLA DE VERDAD función AND
X1=np.array([-1,-1,1,1])
X2=np.array([-1,1,-1,1])
Yd=np.array([-1,-1,-1,1])
#Se inicializa los pesos
np.random.seed(2)
W = np.random.rand(3)

# Se define la tasa de aprendizaje
n=0.01
#cantidad de iteraciones
iter=25
# Se inicializa el vector que contiene las predicciones
yo = np.zeros(len(X1))

output_dir = 'output_images'  # Directorio para guardar las imágenes
zip_filename = 'images.zip'  # Nombre del archivo ZIP
os.makedirs(output_dir, exist_ok=True)


movimientos = iter +1
for i in range(movimientos):



  fig, ax = plt.subplots()
  ax.scatter([1], [1],color = 'black')
  ax.scatter([-1,-1,1], [-1,1,-1],color = 'red')
  ax.plot([-2, 2], [-(W[2]/W[1])-(-2)*(W[0]/W[1]), -(W[2]/W[1])-(2)*(W[0]/W[1])],color = 'black')
  ax.set_title('Recta discriminadora',fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
  ax.set_xlabel("X1", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
  ax.set_ylabel("X2", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
  ax.set_xlim(-2, 2)
  ax.set_ylim(-2, 2)

  image_filename = os.path.join(output_dir, f'imagen{str(i+1).zfill(2)}.jpg')
  plt.savefig(image_filename, format='jpg')
  plt.close(fig)  # Cerrar la figura para liberar memoria



  Contador=np.random.permutation(len(X1))
  for j in range(len(Contador)):
# Se calcula yo, si difiere de Yd se actualiza los pesos
    yo[Contador[j]]=np.sign(X1[Contador[j]]*W[0]+X2[Contador[j]]*W[1]+W[2])
    if abs(yo[Contador[j]]-Yd[Contador[j]]) > 0 :
      deltaW=n*(Yd[Contador[j]]-yo[Contador[j]])*np.array([X1[Contador[j]],X2[Contador[j]],1])
      W=W+deltaW



# Crear un archivo ZIP con las imágenes
shutil.make_archive(zip_filename.replace('.zip', ''), 'zip', output_dir)
# Descargar el archivo ZIP
files.download(zip_filename)

#TABLA DE VERDAD función XOR
X1=np.array([-1,-1,1,1])
X2=np.array([-1,1,-1,1])
Yd=np.array([-1,1,1,-1])
Ydcero=np.array([0,1,1,0])
#Se inicializa los pesos
np.random.seed(2)
W = np.random.rand(3)

# Se define la tasa de aprendizaje
n=0.01
#cantidad de iteraciones
iter=35
# Se inicializa el vector que contiene las predicciones y el error
yo = np.zeros(len(X1))
E = np.zeros(iter+1)

#se calcula el error al inicio

for i in range(len(X1)):
  yo[i]=np.sign(X1[i]*W[0]+X2[i]*W[1]+W[2])
  if yo[i]==-1 :
    yo[i]=0

E[0]=sum((yo-Ydcero)**2)

for i in range(iter):
# El contador permite el orden aleatorio
  Contador=np.random.permutation(len(X1))
  for j in range(len(Contador)):
# Se calcula yo, si difiere de Yd se actualiza los pesos
    yo[Contador[j]]=np.sign(X1[Contador[j]]*W[0]+X2[Contador[j]]*W[1]+W[2])
    if abs(yo[Contador[j]]-Yd[Contador[j]]) > 0 :
      deltaW=n*(Yd[Contador[j]]-yo[Contador[j]])*np.array([X1[Contador[j]],X2[Contador[j]],1])
      W=W+deltaW
    if yo[Contador[j]]==-1:
     yo[Contador[j]]=0
# Se calcula el error
  E[i+1]=sum((yo-Ydcero)**2)
#----------------------------------------------------------------------------------------------------------------------------
print(W)
fig, ax = plt.subplots()
ax.scatter([1], [1],color = 'black')
ax.scatter([-1,-1,1], [-1,1,-1],color = 'red')
ax.plot([-2, 2], [-(W[2]/W[1])-(-2)*(W[0]/W[1]), -(W[2]/W[1])-(2)*(W[0]/W[1])],color = 'black')
ax.set_title('Recta discriminadora',fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
ax.set_xlabel("X1", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
ax.set_ylabel("X2", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
ax.set_xlim(-2, 2)
ax.set_ylim(-2, 2)
plt.show()


fig, ax = plt.subplots()
ax.plot(E,color = 'black')
ax.set_xlabel("iteraciones", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
ax.set_ylabel("Error", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
plt.show()

"""# **Ejercicio 2**"""

#se carga los datos
import pandas as pd
from io import StringIO

data = """
x1 x2 x3 y
1.2 -0.8 0 21.755
1.2 0 -0.8 7.176
1.2 -0.8 1 43.045
0 1.2 -0.8 2.875
7.9 1 0 26.503
0.4 0 2.7 68.568
0 0.4 2.7 61.301
-1.3 3.23 3 23.183
0.4 2.7 0 2.82
0.4 2.7 2 17.654
-1.3 0 3.23 72.512
0 -1.3 3.23 88.184
7.9 1 -2 4.653
1.8 0 1.6 49
0 -2 2 76.852
-0.5 0.6 0 7.871
0 1.8 1.6 18.543
-2 2 0 2.66
-0.5 0.6 2.5 51
7.9 0 1 64.107
-1.3 3.23 0 1.48
0 7.9 1 0.32
-2 0 2 40.131
-2 2 -1 0.995
0 -0.5 0.6 24.974
1.8 1.6 1.3 21.417
-0.5 0 0.6 18.243
1.8 1.6 0 6.914
"""
# Se ua StringIO para leer el string en un Dataframe
df = pd.read_csv(StringIO(data), delim_whitespace=True)
print(df)

# Se obtiene X e y
X = df[['x1', 'x2', 'x3']].values
y = df['y'].values

#Se realiza los Histogramas
bins=10
plt.figure(figsize=(12, 10))

# Histogramas de X
plt.subplot(2, 2, 1)
plt.hist(X[:, 0], bins=bins, color='blue', alpha=0.7)
plt.title('Histograma de x1')
plt.xlabel('x1')
plt.ylabel('Frecuencia')

plt.subplot(2, 2, 2)
plt.hist(X[:, 1], bins=bins, color='blue', alpha=0.7)
plt.title('Histograma de x2')
plt.xlabel('x2')
plt.ylabel('Frecuencia')

plt.subplot(2, 2, 3)
plt.hist(X[:, 2], bins=bins, color='blue', alpha=0.7)
plt.title('Histograma de x3')
plt.xlabel('x3')
plt.ylabel('Frecuencia')

# Histograma de y
plt.subplot(2, 2, 4)
plt.hist(y, bins=bins, color='blue', alpha=0.7)
plt.title('Histograma de y')
plt.xlabel('y')
plt.ylabel('Frecuencia')

# Ajustar el layout
plt.tight_layout()
plt.show()

#Se utiliza un escalar MinMax
from sklearn.preprocessing import MinMaxScaler
# Escalar X
scaler_X = MinMaxScaler(feature_range=(-1, 1))
X_escalado = scaler_X.fit_transform(X)

# Escalar y
scaler_y = MinMaxScaler(feature_range=(-1, 1))
y_escalado = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()
plt.figure(figsize=(12, 10))
bins=10
# Histogramas de X escalado
plt.subplot(2, 2, 1)
plt.hist(X_escalado[:, 0], bins=bins, color='blue', alpha=0.7)
plt.title('Histograma de x1 (escalado MinMax)')
plt.xlabel('x1 (escalado)')
plt.ylabel('Frecuencia')

plt.subplot(2, 2, 2)
plt.hist(X_escalado[:, 1], bins=bins, color='blue', alpha=0.7)
plt.title('Histograma de x2 (escalado MinMax)')
plt.xlabel('x2 (escalado)')
plt.ylabel('Frecuencia')

plt.subplot(2, 2, 3)
plt.hist(X_escalado[:, 2], bins=bins, color='blue', alpha=0.7)
plt.title('Histograma de x3 (escalado MinMax)')
plt.xlabel('x3 (escalado)')
plt.ylabel('Frecuencia')

# Histograma de y escalado
plt.subplot(2, 2, 4)
plt.hist(y_escalado, bins=bins, color='blue', alpha=0.7)
plt.title('Histograma de y (escalado MinMax)')
plt.xlabel('y (escalado)')
plt.ylabel('Frecuencia')

plt.tight_layout()
plt.show()

#Perceptrón lineal
#Se inicializa los pesos
np.random.seed(2)
W = np.random.rand(4)
# Se define la tasa de aprendizaje
n=0.01
#cantidad de iteraciones
iter=100
# Se inicializa el vector que contiene las predicciones y el error
yo = np.zeros(len(X_escalado))
E = np.zeros(iter+1)
#se define la funcion que calcula el error

def calculo_error(A, B):
    diferencia_al_cuadrado = (A - B) ** 2
    # Sumar las diferencias al cuadrado
    resultado = (np.sum(diferencia_al_cuadrado))*0.5
    return resultado

#se calcula el error al inicio

for i in range(len(X_escalado)):
  yo[i]=X_escalado[i,0]*W[0]+X_escalado[i,1]*W[1]+X_escalado[i,2]*W[2]+W[3]

y0_descalado = scaler_y.inverse_transform(yo.reshape(-1, 1)).flatten()

E[0]=calculo_error(y0_descalado,y)

for i in range(iter):
# El contador permite el orden aleatorio
  Contador=np.random.permutation(len(X_escalado))
  for j in range(len(Contador)):
# Se calcula yo, si difiere de y_escalado se actualiza los pesos
    yo[Contador[j]]=X_escalado[Contador[j],0]*W[0]+X_escalado[Contador[j],1]*W[1]+X_escalado[Contador[j],2]*W[2]+W[3]

    if abs(yo[Contador[j]]-y_escalado[Contador[j]]) > 0 :


      deltaW=n*(y_escalado[Contador[j]]-yo[Contador[j]])*np.array([X_escalado[Contador[j],0],X_escalado[Contador[j],1],X_escalado[Contador[j],2],1])
      W=W+deltaW

  y0_descalado = scaler_y.inverse_transform(yo.reshape(-1, 1)).flatten()
# Se calcula el error
  E[i+1]=calculo_error(y0_descalado,y)



fig, ax = plt.subplots()
ax.plot(E,color = 'black')
ax.set_xlabel("iteraciones", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
ax.set_ylabel("Error", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
plt.ylim(0, 15000)
plt.show()

#se grafica el grafico de dispersión de la predicción y el valor real

linea = np.linspace(min(y0_descalado), max(y0_descalado))
plt.figure(figsize=(8, 8))
plt.scatter(y0_descalado,y, c='black')
plt.plot(linea,linea, color='black', linestyle='--')
plt.xlabel('Valor predicho', fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
plt.ylabel('Valor real', fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
plt.title('Gráfico de dispersión del valor predicho frente al valor real', fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
plt.grid(True)
plt.show()

#influencia de la semilla
fig, ax = plt.subplots()
for k in range(100):

  np.random.seed(k)
  W = np.random.rand(4)
  # Se define la tasa de aprendizaje
  n=0.01
  #cantidad de iteraciones
  iter=100
  # Se inicializa el vector que contiene las predicciones y el error
  yo = np.zeros(len(X_escalado))
  E = np.zeros(iter+1)
  #se define la funcion que calcula el error

  def calculo_error(A, B):
      diferencia_al_cuadrado = (A - B) ** 2
      # Sumar las diferencias al cuadrado
      resultado = (np.sum(diferencia_al_cuadrado))*0.5
      return resultado

  #se calcula el error al inicio

  for i in range(len(X_escalado)):
    yo[i]=X_escalado[i,0]*W[0]+X_escalado[i,1]*W[1]+X_escalado[i,2]*W[2]+W[3]

  y0_descalado = scaler_y.inverse_transform(yo.reshape(-1, 1)).flatten()

  E[0]=calculo_error(y0_descalado,y)

  for i in range(iter):
  # El contador permite el orden aleatorio
    Contador=np.random.permutation(len(X_escalado))
    for j in range(len(Contador)):
  # Se calcula yo, si difiere de y_escalado se actualiza los pesos
      yo[Contador[j]]=X_escalado[Contador[j],0]*W[0]+X_escalado[Contador[j],1]*W[1]+X_escalado[Contador[j],2]*W[2]+W[3]

      if abs(yo[Contador[j]]-y_escalado[Contador[j]]) > 0 :


        deltaW=n*(y_escalado[Contador[j]]-yo[Contador[j]])*np.array([X_escalado[Contador[j],0],X_escalado[Contador[j],1],X_escalado[Contador[j],2],1])
        W=W+deltaW

    y0_descalado = scaler_y.inverse_transform(yo.reshape(-1, 1)).flatten()
  # Se calcula el error
    E[i+1]=calculo_error(y0_descalado,y)



  plt.title('Influencia de la semilla', fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
  ax.plot(E,color = 'black')
  ax.set_xlabel("iteraciones", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
  ax.set_ylabel("Error", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
  plt.ylim(0, 15000)

plt.show()

#Perceptrón no lineal  funcion tanh
#Se inicializa los pesos
np.random.seed(2)
W = np.random.rand(4)
# Se define la tasa de aprendizaje
n=0.05
#cantidad de iteraciones
iter=500
# Se inicializa el vector que contiene las predicciones y el error
yo = np.zeros(len(X_escalado))
E = np.zeros(iter+1)
#se define la funcion que calcula el error

def calculo_error(A, B):
    diferencia_al_cuadrado = (A - B) ** 2
    # Sumar las diferencias al cuadrado
    resultado = (np.sum(diferencia_al_cuadrado))*0.5
    return resultado

#se calcula el error al inicio
for i in range(len(X_escalado)):
  yo[i]= np.tanh(beta*(X_escalado[i,0]*W[0]+X_escalado[i,1]*W[1]+X_escalado[i,2]*W[2]+W[3]))

y0_descalado = scaler_y.inverse_transform(yo.reshape(-1, 1)).flatten()

E[0]=calculo_error(y0_descalado,y)


#se define beta
beta=1


for i in range(iter):
# El contador permite el orden aleatorio
  Contador=np.random.permutation(len(X_escalado))
  for j in range(len(Contador)):
# Se calcula yo, si difiere de y_escalado se actualiza los pesos
    yo[Contador[j]]=np.tanh(beta*(X_escalado[Contador[j],0]*W[0]+X_escalado[Contador[j],1]*W[1]+X_escalado[Contador[j],2]*W[2]+W[3]))

    if abs(yo[Contador[j]]-y_escalado[Contador[j]]) > 0 :

      g=beta*(1-((np.tanh(beta*(X_escalado[Contador[j],0]*W[0]+X_escalado[Contador[j],1]*W[1]+X_escalado[Contador[j],2]*W[2]+W[3])))** 2))
      deltaW=n*(y_escalado[Contador[j]]-yo[Contador[j]])*g*np.array([X_escalado[Contador[j],0],X_escalado[Contador[j],1],X_escalado[Contador[j],2],1])

      W=W+deltaW

  y0_descalado = scaler_y.inverse_transform(yo.reshape(-1, 1)).flatten()
# Se calcula el error
  E[i+1]=calculo_error(y0_descalado,y)



fig, ax = plt.subplots()
ax.plot(E,color = 'black')
ax.set_xlabel("iteraciones", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
ax.set_ylabel("Error", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})

plt.show()

#se grafica el grafico de dispersión de la predicción y el valor real

linea = np.linspace(min(y0_descalado), max(y0_descalado))
plt.figure(figsize=(8, 8))
plt.scatter(y0_descalado,y, c='black')
plt.plot(linea,linea, color='black', linestyle='--')
plt.xlabel('Valor predicho', fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
plt.ylabel('Valor real', fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
plt.title('Gráfico de dispersión del valor predicho frente al valor real', fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
plt.grid(True)
plt.show()

fig, ax = plt.subplots()
for k in range(10):
#Perceptrón no lineal  funcion tanh
#Se inicializa los pesos
  np.random.seed(k)
  W = np.random.rand(4)
  # Se define la tasa de aprendizaje
  n=0.05
  #cantidad de iteraciones
  iter=500
  # Se inicializa el vector que contiene las predicciones y el error
  yo = np.zeros(len(X_escalado))
  E = np.zeros(iter+1)
  #se define la funcion que calcula el error

  def calculo_error(A, B):
      diferencia_al_cuadrado = (A - B) ** 2
      # Sumar las diferencias al cuadrado
      resultado = (np.sum(diferencia_al_cuadrado))*0.5
      return resultado

#se calcula el error al inicio
  for i in range(len(X_escalado)):
    yo[i]=np.tanh(beta*(X_escalado[i,0]*W[0]+X_escalado[i,1]*W[1]+X_escalado[i,2]*W[2]+W[3]))

  y0_descalado = scaler_y.inverse_transform(yo.reshape(-1, 1)).flatten()

  E[0]=calculo_error(y0_descalado,y)


  #se define beta
  beta=1


  for i in range(iter):
  # El contador permite el orden aleatorio
    Contador=np.random.permutation(len(X_escalado))
    for j in range(len(Contador)):
  # Se calcula yo, si difiere de y_escalado se actualiza los pesos
      yo[Contador[j]]=np.tanh(beta*(X_escalado[Contador[j],0]*W[0]+X_escalado[Contador[j],1]*W[1]+X_escalado[Contador[j],2]*W[2]+W[3]))

      if abs(yo[Contador[j]]-y_escalado[Contador[j]]) > 0 :

        g=beta*(1-((np.tanh(beta*(X_escalado[Contador[j],0]*W[0]+X_escalado[Contador[j],1]*W[1]+X_escalado[Contador[j],2]*W[2]+W[3])))** 2))
        deltaW=n*(y_escalado[Contador[j]]-yo[Contador[j]])*g*np.array([X_escalado[Contador[j],0],X_escalado[Contador[j],1],X_escalado[Contador[j],2],1])

        W=W+deltaW

    y0_descalado = scaler_y.inverse_transform(yo.reshape(-1, 1)).flatten()
  # Se calcula el error
    E[i+1]=calculo_error(y0_descalado,y)




  ax.plot(E,color = 'black')
  ax.set_xlabel("iteraciones", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
  ax.set_ylabel("Error", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
  plt.ylim(0, 200)
  plt.xlim(30,500)

plt.show()

# se grafica tanh
x_tan = np.linspace(-5, 5, 100)

beta=0.3
y_tan = np.tanh(beta*x)

plt.figure(figsize=(6, 6))
plt.plot(x_tan,y_tan, color='black')
plt.xlabel('x', fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
plt.ylabel('tanh(x)', fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})

plt.ylim(-1.5, 1.5)
plt.xlim(-5, 5)

#influencia de beta
puntos=10
puntos_beta=np.linspace(-8,8,puntos)
fig, ax = plt.subplots()

for k in range(puntos):
#Perceptrón no lineal  funcion tanh
#Se inicializa los pesos
  beta=puntos_beta[k]
  np.random.seed(2)
  W = np.random.rand(4)
  # Se define la tasa de aprendizaje
  n=0.05
  #cantidad de iteraciones
  iter=500
  # Se inicializa el vector que contiene las predicciones y el error
  yo = np.zeros(len(X_escalado))
  E = np.zeros(iter+1)
  #se define la funcion que calcula el error

  def calculo_error(A, B):
      diferencia_al_cuadrado = (A - B) ** 2
      # Sumar las diferencias al cuadrado
      resultado = (np.sum(diferencia_al_cuadrado))*0.5
      return resultado

#se calcula el error al inicio
  for i in range(len(X_escalado)):
    yo[i]=  X_escalado[i,0]*W[0]+X_escalado[i,1]*W[1]+X_escalado[i,2]*W[2]+W[3]

  y0_descalado = scaler_y.inverse_transform(yo.reshape(-1, 1)).flatten()

  E[0]=calculo_error(y0_descalado,y)



  for i in range(iter):
  # El contador permite el orden aleatorio
    Contador=np.random.permutation(len(X_escalado))
    for j in range(len(Contador)):
  # Se calcula yo, si difiere de y_escalado se actualiza los pesos
      yo[Contador[j]]=np.tanh(beta*(X_escalado[Contador[j],0]*W[0]+X_escalado[Contador[j],1]*W[1]+X_escalado[Contador[j],2]*W[2]+W[3]))

      if abs(yo[Contador[j]]-y_escalado[Contador[j]]) > 0 :

        g=beta*(1-((np.tanh(beta*(X_escalado[Contador[j],0]*W[0]+X_escalado[Contador[j],1]*W[1]+X_escalado[Contador[j],2]*W[2]+W[3])))** 2))
        deltaW=n*(y_escalado[Contador[j]]-yo[Contador[j]])*g*np.array([X_escalado[Contador[j],0],X_escalado[Contador[j],1],X_escalado[Contador[j],2],1])

        W=W+deltaW

    y0_descalado = scaler_y.inverse_transform(yo.reshape(-1, 1)).flatten()
  # Se calcula el error
    E[i+1]=calculo_error(y0_descalado,y)




  ax.plot(E,color = 'black')
  ax.set_xlabel("iteraciones", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
  ax.set_ylabel("Error", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
  plt.ylim(0, 200)
  plt.xlim(0,500)

plt.show()

#item 2 , analisis de la capacidad generalizadora
from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(X_escalado,y, test_size=0.3,random_state=500)

#Perceptrón no lineal  funcion tanh
#Se inicializa los pesos
np.random.seed(2)
W = np.random.rand(4)
# Se define la tasa de aprendizaje
n=0.01
#cantidad de iteraciones
iter=1600
# Se inicializa el vector que contiene las predicciones y el error
y_train_o = np.zeros(len(X_train))
y_val_o = np.zeros(len(X_val))
E_entenamiento= np.zeros(iter+1)
E_validacion=np.zeros(iter+1)
#se define la funcion que calcula el error

def calculo_error(A, B):
    diferencia_al_cuadrado = (A - B) ** 2
    # Sumar las diferencias al cuadrado
    resultado = (np.sum(diferencia_al_cuadrado))*0.5
    return resultado

#se define beta
beta=1


#se calcula el error al inicio
for i in range(len(X_val)):
  y_val_o[i]= np.tanh(beta*(X_val[i,0]*W[0]+X_val[i,1]*W[1]+X_val[i,2]*W[2]+W[3]))
y_val_0_descalado = scaler_y.inverse_transform(y_val_o.reshape(-1, 1)).flatten()
E_validacion[0]=calculo_error(y_val_0_descalado,y_val)


for i in range(len(X_train)):
  y_train_o[i]= np.tanh(beta*(X_train[i,0]*W[0]+X_train[i,1]*W[1]+X_train[i,2]*W[2]+W[3]))
y_train_0_descalado = scaler_y.inverse_transform(y_train_o.reshape(-1, 1)).flatten()
E_entenamiento[0]=calculo_error(y_train_0_descalado,y_train)


y_train_escalado=scaler_y.transform(y_train.reshape(-1, 1)).flatten()
y_val_escalado=scaler_y.transform(y_val.reshape(-1, 1)).flatten()


for i in range(iter):

  for j in range(len(X_val)):
    y_val_o[j]= np.tanh(beta*(X_val[j,0]*W[0]+X_val[j,1]*W[1]+X_val[j,2]*W[2]+W[3]))
  y_val_0_descalado = scaler_y.inverse_transform(y_val_o.reshape(-1, 1)).flatten()
  E_validacion[i+1]=calculo_error(y_val_0_descalado,y_val)

# El contador permite el orden aleatorio
  Contador=np.random.permutation(len(X_train))
  for j in range(len(Contador)):
# Se calcula yo, si difiere de y_escalado se actualiza los pesos
    y_train_o[Contador[j]]=np.tanh(beta*(X_train[Contador[j],0]*W[0]+X_train[Contador[j],1]*W[1]+X_train[Contador[j],2]*W[2]+W[3]))


    if abs(y_train_o[Contador[j]]-y_train_escalado[Contador[j]]) > 0 :

      g=beta*(1-((np.tanh(beta*(X_train[Contador[j],0]*W[0]+X_train[Contador[j],1]*W[1]+X_train[Contador[j],2]*W[2]+W[3])))** 2))
      deltaW=n*(y_train_escalado[Contador[j]]-y_train_o[Contador[j]])*g*np.array([X_train[Contador[j],0],X_train[Contador[j],1],X_train[Contador[j],2],1])

      W=W+deltaW

  y_train_o_descalado = scaler_y.inverse_transform(y_train_o.reshape(-1, 1)).flatten()
# Se calcula el error
  E_entenamiento[i+1]=calculo_error(y_train_o_descalado,y_train)



fig, ax = plt.subplots()
ax.plot(E_entenamiento,color = 'red', label='Entrenamiento')
ax.plot(E_validacion,color = 'blue', label='Validación')

ax.set_xlabel("iteraciones", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
ax.set_ylabel("Error", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
ax.legend()
plt.ylim(0,200)
plt.xlim(0,iter)
plt.show()

#se evalua si es posible encontra sun bias que permita que el split no interfiera en la capacidad de generalizacion del modelo

from sklearn.model_selection import train_test_split

pasos=100



Err_val_semilla=np.zeros(pasos)
Err_train_semilla=np.zeros(pasos)



for k in range(pasos):

  X_train, X_val, y_train, y_val = train_test_split(X_escalado,y, test_size=0.1,random_state=k*10)

#Perceptrón no lineal  funcion tanh
#Se inicializa los pesos
  np.random.seed(2)
  W = np.random.rand(4)
# Se define la tasa de aprendizaje
  n=0.05
#cantidad de iteraciones
  iter=600
# Se inicializa el vector que contiene las predicciones y el error
  y_train_o = np.zeros(len(X_train))
  y_val_o = np.zeros(len(X_val))
  E_entenamiento= np.zeros(iter+1)
  E_validacion=np.zeros(iter+1)
  #se define la funcion que calcula el error

  def calculo_error(A, B):
      diferencia_al_cuadrado = (A - B) ** 2
      # Sumar las diferencias al cuadrado
      resultado = (np.sum(diferencia_al_cuadrado))*0.5
      return resultado

#se define beta
  beta=2


#se calcula el error al inicio
  for i in range(len(X_val)):
    y_val_o[i]= np.tanh(beta*(X_val[i,0]*W[0]+X_val[i,1]*W[1]+X_val[i,2]*W[2]+W[3]))
  y_val_0_descalado = scaler_y.inverse_transform(y_val_o.reshape(-1, 1)).flatten()
  E_validacion[0]=calculo_error(y_val_0_descalado,y_val)


  for i in range(len(X_train)):
    y_train_o[i]= np.tanh(beta*(X_train[i,0]*W[0]+X_train[i,1]*W[1]+X_train[i,2]*W[2]+W[3]))
  y_train_0_descalado = scaler_y.inverse_transform(y_train_o.reshape(-1, 1)).flatten()
  E_entenamiento[0]=calculo_error(y_train_0_descalado,y_train)


  y_train_escalado=scaler_y.transform(y_train.reshape(-1, 1)).flatten()
  y_val_escalado=scaler_y.transform(y_val.reshape(-1, 1)).flatten()


  for i in range(iter):

    for j in range(len(X_val)):
      y_val_o[j]= np.tanh(beta*(X_val[j,0]*W[0]+X_val[j,1]*W[1]+X_val[j,2]*W[2]+W[3]))
    y_val_0_descalado = scaler_y.inverse_transform(y_val_o.reshape(-1, 1)).flatten()
    E_validacion[i+1]=calculo_error(y_val_0_descalado,y_val)

# El contador permite el orden aleatorio
    Contador=np.random.permutation(len(X_train))
    for j in range(len(Contador)):
# Se calcula yo, si difiere de y_escalado se actualiza los pesos
      y_train_o[Contador[j]]=np.tanh(beta*(X_train[Contador[j],0]*W[0]+X_train[Contador[j],1]*W[1]+X_train[Contador[j],2]*W[2]+W[3]))


      if abs(y_train_o[Contador[j]]-y_train_escalado[Contador[j]]) > 0 :

        g=beta*(1-((np.tanh(beta*(X_train[Contador[j],0]*W[0]+X_train[Contador[j],1]*W[1]+X_train[Contador[j],2]*W[2]+W[3])))** 2))
        deltaW=n*(y_train_escalado[Contador[j]]-y_train_o[Contador[j]])*g*np.array([X_train[Contador[j],0],X_train[Contador[j],1],X_train[Contador[j],2],1])

        W=W+deltaW

    y_train_o_descalado = scaler_y.inverse_transform(y_train_o.reshape(-1, 1)).flatten()
# Se calcula el error
    E_entenamiento[i+1]=calculo_error(y_train_o_descalado,y_train)

  Err_val_semilla[k]=E_validacion[-1]
  Err_train_semilla[k]=E_entenamiento[-1]



fig, ax = plt.subplots()
ax.plot(Err_train_semilla,color = 'red', label='Entrenamiento')
ax.plot(Err_val_semilla,color = 'blue', label='Validación')

ax.set_xlabel("Escenario con diferente semilla", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
ax.set_ylabel("Error", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
ax.legend()
plt.ylim(0,45)
plt.show()

from sklearn.model_selection import train_test_split
import pandas as pd

pasos=100

indice_max = np.argmax(y)

# Eliminar la fila correspondiente en X e y
X_escalado_nuevo= np.delete(X_escalado, indice_max, axis=0)

y_nuevo= np.delete(y, indice_max)

Err_val_semilla=np.zeros(pasos)
Err_train_semilla=np.zeros(pasos)



for k in range(pasos):

  X_train, X_val, y_train, y_val = train_test_split(X_escalado_nuevo,y_nuevo, test_size=0.1,random_state=k*10)

#Perceptrón no lineal  funcion tanh
#Se inicializa los pesos
  np.random.seed(2)
  W = np.random.rand(4)
# Se define la tasa de aprendizaje
  n=0.05
#cantidad de iteraciones
  iter=600
# Se inicializa el vector que contiene las predicciones y el error
  y_train_o = np.zeros(len(X_train))
  y_val_o = np.zeros(len(X_val))
  E_entenamiento= np.zeros(iter+1)
  E_validacion=np.zeros(iter+1)
  #se define la funcion que calcula el error

  def calculo_error(A, B):
      diferencia_al_cuadrado = (A - B) ** 2
      # Sumar las diferencias al cuadrado
      resultado = (np.sum(diferencia_al_cuadrado))*0.5
      return resultado

#se define beta
  beta=5


#se calcula el error al inicio
  for i in range(len(X_val)):
    y_val_o[i]= np.tanh(beta*(X_val[i,0]*W[0]+X_val[i,1]*W[1]+X_val[i,2]*W[2]+W[3]))
  y_val_0_descalado = scaler_y.inverse_transform(y_val_o.reshape(-1, 1)).flatten()
  E_validacion[0]=calculo_error(y_val_0_descalado,y_val)


  for i in range(len(X_train)):
    y_train_o[i]= np.tanh(beta*(X_train[i,0]*W[0]+X_train[i,1]*W[1]+X_train[i,2]*W[2]+W[3]))
  y_train_0_descalado = scaler_y.inverse_transform(y_train_o.reshape(-1, 1)).flatten()
  E_entenamiento[0]=calculo_error(y_train_0_descalado,y_train)


  y_train_escalado=scaler_y.transform(y_train.reshape(-1, 1)).flatten()
  y_val_escalado=scaler_y.transform(y_val.reshape(-1, 1)).flatten()


  for i in range(iter):

    for j in range(len(X_val)):
      y_val_o[j]= np.tanh(beta*(X_val[j,0]*W[0]+X_val[j,1]*W[1]+X_val[j,2]*W[2]+W[3]))
    y_val_0_descalado = scaler_y.inverse_transform(y_val_o.reshape(-1, 1)).flatten()
    E_validacion[i+1]=calculo_error(y_val_0_descalado,y_val)

# El contador permite el orden aleatorio
    Contador=np.random.permutation(len(X_train))
    for j in range(len(Contador)):
# Se calcula yo, si difiere de y_escalado se actualiza los pesos
      y_train_o[Contador[j]]=np.tanh(beta*(X_train[Contador[j],0]*W[0]+X_train[Contador[j],1]*W[1]+X_train[Contador[j],2]*W[2]+W[3]))


      if abs(y_train_o[Contador[j]]-y_train_escalado[Contador[j]]) > 0 :

        g=beta*(1-((np.tanh(beta*(X_train[Contador[j],0]*W[0]+X_train[Contador[j],1]*W[1]+X_train[Contador[j],2]*W[2]+W[3])))** 2))
        deltaW=n*(y_train_escalado[Contador[j]]-y_train_o[Contador[j]])*g*np.array([X_train[Contador[j],0],X_train[Contador[j],1],X_train[Contador[j],2],1])

        W=W+deltaW

    y_train_o_descalado = scaler_y.inverse_transform(y_train_o.reshape(-1, 1)).flatten()
# Se calcula el error
    E_entenamiento[i+1]=calculo_error(y_train_o_descalado,y_train)

  Err_val_semilla[k]=E_validacion[-1]
  Err_train_semilla[k]=E_entenamiento[-1]



fig, ax = plt.subplots()
ax.plot(Err_train_semilla,color = 'red', label='Entrenamiento')
ax.plot(Err_val_semilla,color = 'blue', label='Validación')

ax.set_xlabel("Escenario con diferente semilla", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
ax.set_ylabel("Error", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
ax.legend()
plt.ylim(0,45)
plt.show()

from sklearn.model_selection import train_test_split


indice_max = np.argmax(y)

# Eliminar la fila correspondiente en X e y
X_escalado_nuevo= np.delete(X_escalado, indice_max, axis=0)

y_nuevo= np.delete(y, indice_max)
X_train, X_val, y_train, y_val = train_test_split(X_escalado_nuevo,y_nuevo, test_size=0.3,random_state=400)

#Perceptrón no lineal  funcion tanh
#Se inicializa los pesos
np.random.seed(99)
W = np.random.rand(4)
# Se define la tasa de aprendizaje
n=0.05
#cantidad de iteraciones
iter=600
# Se inicializa el vector que contiene las predicciones y el error
y_train_o = np.zeros(len(X_train))
y_val_o = np.zeros(len(X_val))
E_entenamiento= np.zeros(iter+1)
E_validacion=np.zeros(iter+1)
#se define la funcion que calcula el error

def calculo_error(A, B):
    diferencia_al_cuadrado = (A - B) ** 2
    # Sumar las diferencias al cuadrado
    resultado = (np.sum(diferencia_al_cuadrado))*0.5
    return resultado

#se define beta
beta=1


#se calcula el error al inicio
for i in range(len(X_val)):
  y_val_o[i]= np.tanh(beta*(X_val[i,0]*W[0]+X_val[i,1]*W[1]+X_val[i,2]*W[2]+W[3]))
y_val_0_descalado = scaler_y.inverse_transform(y_val_o.reshape(-1, 1)).flatten()
E_validacion[0]=calculo_error(y_val_0_descalado,y_val)


for i in range(len(X_train)):
  y_train_o[i]= np.tanh(beta*(X_train[i,0]*W[0]+X_train[i,1]*W[1]+X_train[i,2]*W[2]+W[3]))
y_train_0_descalado = scaler_y.inverse_transform(y_train_o.reshape(-1, 1)).flatten()
E_entenamiento[0]=calculo_error(y_train_0_descalado,y_train)


y_train_escalado=scaler_y.transform(y_train.reshape(-1, 1)).flatten()
y_val_escalado=scaler_y.transform(y_val.reshape(-1, 1)).flatten()


for i in range(iter):

  for j in range(len(X_val)):
    y_val_o[j]= np.tanh(beta*(X_val[j,0]*W[0]+X_val[j,1]*W[1]+X_val[j,2]*W[2]+W[3]))
  y_val_0_descalado = scaler_y.inverse_transform(y_val_o.reshape(-1, 1)).flatten()
  E_validacion[i+1]=calculo_error(y_val_0_descalado,y_val)

# El contador permite el orden aleatorio
  Contador=np.random.permutation(len(X_train))
  for j in range(len(Contador)):
# Se calcula yo, si difiere de y_escalado se actualiza los pesos
    y_train_o[Contador[j]]=np.tanh(beta*(X_train[Contador[j],0]*W[0]+X_train[Contador[j],1]*W[1]+X_train[Contador[j],2]*W[2]+W[3]))


    if abs(y_train_o[Contador[j]]-y_train_escalado[Contador[j]]) > 0 :

      g=beta*(1-((np.tanh(beta*(X_train[Contador[j],0]*W[0]+X_train[Contador[j],1]*W[1]+X_train[Contador[j],2]*W[2]+W[3])))** 2))
      deltaW=n*(y_train_escalado[Contador[j]]-y_train_o[Contador[j]])*g*np.array([X_train[Contador[j],0],X_train[Contador[j],1],X_train[Contador[j],2],1])

      W=W+deltaW

  y_train_o_descalado = scaler_y.inverse_transform(y_train_o.reshape(-1, 1)).flatten()
# Se calcula el error
  E_entenamiento[i+1]=calculo_error(y_train_o_descalado,y_train)



fig, ax = plt.subplots()
ax.plot(E_entenamiento,color = 'red', label='Entrenamiento')
ax.plot(E_validacion,color = 'blue', label='Validación')

ax.set_xlabel("iteraciones", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
ax.set_ylabel("Error", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'black'})
ax.legend()
plt.ylim(0,200)
plt.xlim(0,iter)
plt.show()

#para graficar el sistema
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(10, 6))
ax = fig.add_subplot(111, projection='3d')

# se normaliza los colores según los valores de y
norm = plt.Normalize(y.min(),88)
colors = plt.cm.viridis(norm(y))


ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=colors, s=50)


ax.view_init(elev=20, azim=10)

ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('X3')
ax.set_title('Dispersión 3D de X con colores de acuerdo a y')


mappable = plt.cm.ScalarMappable(norm=norm, cmap='viridis')
mappable.set_array(y)
plt.colorbar(mappable, label='Valores de y')

plt.show()

# Se calcula yo, si difiere de y_escalado se actualiza los pesos


for j in range(len(X_escalado)):
  yo[j]=np.tanh(beta*(X_escalado[j,0]*W[0]+X_escalado[j,1]*W[1]+X_escalado[j,2]*W[2]+W[3]))

y0_descalado = scaler_y.inverse_transform(yo.reshape(-1, 1)).flatten()


print(y0_descalado)

print(y)

#para graficar el sistema
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(10, 6))
ax = fig.add_subplot(111, projection='3d')

# se normaliza los colores según los valores de y
norm = plt.Normalize(y.min(),88)
colors = plt.cm.viridis(norm(y0_descalado))


ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=colors, s=50)


ax.view_init(elev=20, azim=10)

ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('X3')
ax.set_title('Dispersión 3D de X con colores de acuerdo al modelo')


mappable = plt.cm.ScalarMappable(norm=norm, cmap='viridis')
mappable.set_array(y0_descalado)
plt.colorbar(mappable, label='Valores de y')

plt.show()

"""# **Ejercicio 3**"""

#se cargan los datos

matriz = [
    [0, 1, 1, 1, 0],
    [1, 0, 0, 0, 1],
    [1, 0, 0, 1, 1],
    [1, 0, 1, 0, 1],
    [1, 1, 0, 0, 1],
    [1, 0, 0, 0, 1],
    [0, 1, 1, 1, 0],
    [0, 0, 1, 0, 0],
    [0, 1, 1, 0, 0],
    [0, 0, 1, 0, 0],
    [0, 0, 1, 0, 0],
    [0, 0, 1, 0, 0],
    [0, 0, 1, 0, 0],
    [0, 1, 1, 1, 0],
    [0, 1, 1, 1, 0],
    [1, 0, 0, 0, 1],
    [0, 0, 0, 0, 1],
    [0, 0, 0, 1, 0],
    [0, 0, 1, 0, 0],
    [0, 1, 0, 0, 0],
    [1, 1, 1, 1, 1],
    [0, 1, 1, 1, 0],
    [1, 0, 0, 0, 1],
    [0, 0, 0, 0, 1],
    [0, 0, 1, 1, 0],
    [0, 0, 0, 0, 1],
    [1, 0, 0, 0, 1],
    [0, 1, 1, 1, 0],
    [0, 0, 0, 1, 0],
    [0, 0, 1, 1, 0],
    [0, 1, 0, 1, 0],
    [1, 0, 0, 1, 0],
    [1, 1, 1, 1, 1],
    [0, 0, 0, 1, 0],
    [0, 0, 0, 1, 0],
    [1, 1, 1, 1, 1],
    [1, 0, 0, 0, 0],
    [1, 1, 1, 1, 0],
    [0, 0, 0, 0, 1],
    [0, 0, 0, 0, 1],
    [1, 0, 0, 0, 1],
    [0, 1, 1, 1, 0],
    [0, 0, 1, 1, 0],
    [0, 1, 0, 0, 0],
    [1, 0, 0, 0, 0],
    [1, 1, 1, 1, 0],
    [1, 0, 0, 0, 1],
    [1, 0, 0, 0, 1],
    [0, 1, 1, 1, 0],
    [1, 1, 1, 1, 1],
    [0, 0, 0, 0, 1],
    [0, 0, 0, 1, 0],
    [0, 0, 1, 0, 0],
    [0, 1, 0, 0, 0],
    [0, 1, 0, 0, 0],
    [0, 1, 0, 0, 0],
    [0, 1, 1, 1, 0],
    [1, 0, 0, 0, 1],
    [1, 0, 0, 0, 1],
    [0, 1, 1, 1, 0],
    [1, 0, 0, 0, 1],
    [1, 0, 0, 0, 1],
    [0, 1, 1, 1, 0],
    [0, 1, 1, 1, 0],
    [1, 0, 0, 0, 1],
    [1, 0, 0, 0, 1],
    [0, 1, 1, 1, 1],
    [0, 0, 0, 0, 1],
    [0, 0, 0, 1, 0],
    [0, 1, 1, 0, 0]
]

# se divide la matriz en grupos de 7 filas
n = 7
chunks = [matriz[i:i + n] for i in range(0, len(matriz), n)]

for idx, chunk in enumerate(chunks, start=0):
    print(f"x{idx} =", chunk)
    print()

#se aplana para pasar a tener vectores
for idx, chunk in enumerate(chunks, start=0):
    vector_name = f"x{idx}"
    vector = [item for sublist in chunk for item in sublist]
    print(f"{vector_name} =", vector)
    print()

chunks = [matriz[i:i + n] for i in range(0, len(matriz), n)]

# Crear la matriz X y agregar los vectores como filas
X = []
for chunk in chunks:
    vector = [item for sublist in chunk for item in sublist]  # Aplanar la lista
    X.append(vector)

y =[1,0,1, 0, 1, 0, 1,0,1,0]
X= np.array(X)

# Se aprende la función XOR
#XOR de dos entradas
#TABLA DE VERDAD
X1=np.array([-1,-1,1,1])
X2=np.array([-1,1,-1,1])
Y=np.array([-1,1,1,-1])
#Se normaliza el vector Y, es un procedimiento habitual en machine learning, especialmente en redes neuronales
# ya que la función de transferencia no puede tomar valores negativos. La entrada se normaliza ya que al no contar las neuronas ocultas con
#bias, el modelo presentaria problemas para converger.
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0,1))
Yd= scaler.fit_transform(Y.reshape(-1, 1))
X1=scaler.fit_transform(X1.reshape(-1, 1))
X2=scaler.fit_transform(X2.reshape(-1, 1))
#Arquitectura de la red
Cantidad_Entradas=2
Neuronas_C_O=8
Neuronas_C_S=1
#Inicialización de los pesos sinápticos
# Se fija la semilla para que el programa sea reproducible
np.random.seed(1)
#ENTRADA-CAPA OCULTA
WE_CO=-1+2*np.random.rand(Cantidad_Entradas,Neuronas_C_O)
#CAPA OCULTA-SALIDA
WCO_S=-1+2*np.random.rand(Neuronas_C_O)
iter=9000
Aprendizaje=0.25
#se crea una matriz que contenga todas las entradas para que luego sea mas facil acceder a esa información
G=np.zeros((len(X1),Cantidad_Entradas))
G[:,0]=X1.transpose()
G[:,1]=X2.transpose()
#vector que guarda la salida de la neurona de la capa oculta
SCO = np.zeros(Neuronas_C_O)
# Salida de la red y error cuadratico
ECUAD= np.zeros(iter)
SALIDA= np.zeros(len(X1))
#comienzo del modelo
for t in range(iter):
# Al añadir este bucle permite que el orden de entrenamiento(patron empleado) sea aleatorio
  Contador=np.random.permutation(len(X1))
  for i in range(len(Contador)):
# FEEDFORWARD
#para cada neurona de la capa oculta se calcula su salida multiplicando el peso correspondiente por la entrada, realizando la suma
#y la función de transferencia
    for h in range(Neuronas_C_O):
      # suma de los pesos por la entrada
      valor=np.dot(G[Contador[i],:],WE_CO[:,h])
      #se aplica la función de transferencia
      SCO[h]= 1/(1+math.exp(-valor))
    # Se calcula la salida de la red de forma similar que para las neuronas de la capa interna
    valor2= np.dot(SCO,WCO_S)
    SALIDA[Contador[i]]= 1/(1+math.exp(-valor2))
# BACKPROPAGATION
# ERROR SALIDA,OCULTA
    delta=(Yd[Contador[i]]-SALIDA[Contador[i]])*(1-SALIDA[Contador[i]])*SALIDA[Contador[i]]
# se crea una copia de WCO_S para utilizarlo en la siguiente sección
    WCO_SOLD=WCO_S.copy()
    WCO_S=WCO_S+Aprendizaje*delta*SCO
#ERROR OCULTA,ENTRADA
    deltaOE=SCO *(1-SCO)*(delta*WCO_SOLD)
# A es un vector intermedio
    for j in range(len(deltaOE)):
      WE_CO[:,j]=WE_CO[:,j]+ Aprendizaje*deltaOE[j] * G[Contador[i],:]
# Se calcula el error cuadratico
  a=(np.transpose(Yd)-SALIDA)**2
  b=sum(a.reshape(-1,1))
  ECUAD[t]=(1/2)*sum(b)

fig, ax = plt.subplots()
ax.plot(ECUAD)
ax.set_xlabel("iteraciones", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})
ax.set_ylabel("Error cuadrático", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})
plt.show()
print(WE_CO)

# Se aplica el multicapa para clasificar la paridad

#Se considera que no tiene bias

#Arquitectura de la red
Cantidad_Entradas=35
Neuronas_C_O=8
Neuronas_C_S=1
#Inicialización de los pesos sinápticos
np.random.seed(1)
#ENTRADA-CAPA OCULTA
WE_CO=-1+2*np.random.rand(Cantidad_Entradas,Neuronas_C_O)
#CAPA OCULTA-SALIDA
WCO_S=-1+2*np.random.rand(Neuronas_C_O)
iter=900
Aprendizaje=0.25
#se crea una matriz que contenga todas las entradas para que luego sea mas facil acceder a esa información
G=X


#vector que guarda la salida de la neurona de la capa oculta
SCO = np.zeros(Neuronas_C_O)
# Salida de la red y error cuadratico
ECUAD= np.zeros(iter)
SALIDA= np.zeros(len(y))
#comienzo del modelo
for t in range(iter):
# Al añadir este bucle permite que el orden de entrenamiento(patron empleado) sea aleatorio
  Contador=np.random.permutation(len(y))
  for i in range(len(Contador)):
# FEEDFORWARD
#para cada neurona de la capa oculta se calcula su salida multiplicando el peso correspondiente por la entrada, realizando la suma
#y la función de transferencia
    for h in range(Neuronas_C_O):
      # suma de los pesos por la entrada
      valor=np.dot(G[Contador[i],:],WE_CO[:,h])
      #se aplica la función de transferencia
      SCO[h]= 1/(1+math.exp(-valor))
    # Se calcula la salida de la red de forma similar que para las neuronas de la capa interna
    valor2= np.dot(SCO,WCO_S)
    SALIDA[Contador[i]]= 1/(1+math.exp(-valor2))
# BACKPROPAGATION
# ERROR SALIDA,OCULTA
    delta=(y[Contador[i]]-SALIDA[Contador[i]])*(1-SALIDA[Contador[i]])*SALIDA[Contador[i]]
# se crea una copia de WCO_S para utilizarlo en la siguiente sección
    WCO_SOLD=WCO_S.copy()
    WCO_S=WCO_S+Aprendizaje*delta*SCO
#ERROR OCULTA,ENTRADA
    deltaOE=SCO *(1-SCO)*(delta*WCO_SOLD)
# A es un vector intermedio
    for j in range(len(deltaOE)):
      WE_CO[:,j]=WE_CO[:,j]+ Aprendizaje*deltaOE[j] * G[Contador[i],:]
# Se calcula el error cuadratico
  a=(np.transpose(y)-SALIDA)**2
  b=sum(a.reshape(-1,1))
  ECUAD[t]=(1/2)*sum(b)

fig, ax = plt.subplots()
ax.plot(ECUAD)
ax.set_xlabel("iteraciones", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})
ax.set_ylabel("Error cuadrático", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})
plt.ylim(0)
plt.show()


print(SALIDA)

# Se aplica el multicapa para clasificar la paridad, se repite el codigo para calcular el accuracy
#Se considera que no tiene bias
#Arquitectura de la red
Cantidad_Entradas=35
Neuronas_C_O=8
Neuronas_C_S=1
#Inicialización de los pesos sinápticos
np.random.seed(1)
#ENTRADA-CAPA OCULTA
WE_CO=-1+2*np.random.rand(Cantidad_Entradas,Neuronas_C_O)
#CAPA OCULTA-SALIDA
WCO_S=-1+2*np.random.rand(Neuronas_C_O)
iter=900
Aprendizaje=0.025
#se crea una matriz que contenga todas las entradas para que luego sea mas facil acceder a esa información
G=X
#vector que guarda la salida de la neurona de la capa oculta
SCO = np.zeros(Neuronas_C_O)
# Salida de la red y error cuadratico
ECUAD= np.zeros(iter)
Accuracy= np.zeros(iter)
SALIDA= np.zeros(len(y))
#comienzo del modelo
for t in range(iter):
# Al añadir este bucle permite que el orden de entrenamiento(patron empleado) sea aleatorio
  Contador=np.random.permutation(len(y))
  for i in range(len(Contador)):
# FEEDFORWARD
#para cada neurona de la capa oculta se calcula su salida multiplicando el peso correspondiente por la entrada, realizando la suma
#y la función de transferencia
    for h in range(Neuronas_C_O):
      # suma de los pesos por la entrada
      valor=np.dot(G[Contador[i],:],WE_CO[:,h])
      #se aplica la función de transferencia
      SCO[h]= 1/(1+math.exp(-valor))
    # Se calcula la salida de la red de forma similar que para las neuronas de la capa interna
    valor2= np.dot(SCO,WCO_S)
    SALIDA[Contador[i]]= 1/(1+math.exp(-valor2))
# BACKPROPAGATION
# ERROR SALIDA,OCULTA
    delta=(y[Contador[i]]-SALIDA[Contador[i]])*(1-SALIDA[Contador[i]])*SALIDA[Contador[i]]
# se crea una copia de WCO_S para utilizarlo en la siguiente sección
    WCO_SOLD=WCO_S.copy()
    WCO_S=WCO_S+Aprendizaje*delta*SCO
#ERROR OCULTA,ENTRADA
    deltaOE=SCO *(1-SCO)*(delta*WCO_SOLD)
# A es un vector intermedio
    for j in range(len(deltaOE)):
      WE_CO[:,j]=WE_CO[:,j]+ Aprendizaje*deltaOE[j] * G[Contador[i],:]
# Se calcula el error cuadratico
  a=(np.transpose(y)-SALIDA)**2
  b=sum(a.reshape(-1,1))
  ECUAD[t]=(1/2)*sum(b)

  y_binario = [1 if val > 0.5 else 0 for val in SALIDA]
  Accuracy[t]=np.mean(np.array(y_binario) == np.array(y))



fig, ax = plt.subplots()
ax.plot(Accuracy)
ax.set_xlabel("iteraciones", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})
ax.set_ylabel("Accuracy", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})

plt.show()
print(SALIDA)

#se repite el codigo para evaluar el error cuadratico en funcion a dos pesos
#Los pesos elegidos para graficar son los correspondiente a la primer neurona
#El rango de valores se elige de manera de poder visualizar el gráfico de forma clara y que contengan el valor óptimo
W1_1=np.arange(start=-4, stop=4, step=0.05)
W1_2=np.arange(start=-4, stop=4, step=0.05)
# H contiene todas las combinaciones de los pesos elegidos y el error correspondiente para cada combinación
H=np.zeros((len(W1_1)*len(W1_2),3))
WE_CO_nuevo=np.zeros((Cantidad_Entradas,Neuronas_C_O))
WE_CO_nuevo=WE_CO.copy()
#Iteración funciona como un contador, para determinar la posición en H
iteracion=0
#Con dos bucles for se hace las combinaciones de los dos pesos
for a in range(len(W1_1)):
  for b in range(len(W1_2)):
    WE_CO_nuevo[0,0]=W1_1[a]
    WE_CO_nuevo[1,0]=W1_2[b]
    Contador=np.random.permutation(len(y))
    for i in range(len(Contador)):
      for h in range(Neuronas_C_O):
        valor=np.dot(G[Contador[i],:],WE_CO_nuevo[:,h])
        SCO[h]= 1/(1+math.exp(-valor))
      valor2= np.dot(SCO,WCO_S)
      SALIDA[Contador[i]]= 1/(1+math.exp(-valor2))
    c=(np.transpose(y)-SALIDA)**2
    d=sum(c.reshape(-1,1))
    E2=(1/2)*sum(d)


    H[iteracion,0]=W1_1[a]
    H[iteracion,1]=W1_2[b]
    H[iteracion,2]=E2
    iteracion=iteracion+1

import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm
from matplotlib.ticker import MaxNLocator



fig = plt.figure(figsize=(14,6))
ax = fig.add_subplot(111, projection='3d')

surf=ax.plot_trisurf(H[:,0],H[:,1],H[:,2], linewidth=0, antialiased=False,cmap=cm.jet)
fig.colorbar(surf)

ax.view_init(20,125)
ax.set_title('error cuadratico vs pesos sinápticos',fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})
ax.set_xlabel("W11", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})
ax.set_ylabel("W12", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})
ax.set_zlabel("E2", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})
fig.tight_layout()
plt.show()

#se repite el codigo para evaluar el accuracy en funcion a dos pesos
#Los pesos elegidos para graficar son los correspondiente a la primer neurona
#El rango de valores se elige de manera de poder visualizar el gráfico de forma clara y que contengan el valor óptimo
W1_1=np.arange(start=-4, stop=4, step=0.05)
W1_2=np.arange(start=-4, stop=4, step=0.05)
# H contiene todas las combinaciones de los pesos elegidos y el error correspondiente para cada combinación
H=np.zeros((len(W1_1)*len(W1_2),3))
WE_CO_nuevo=np.zeros((Cantidad_Entradas,Neuronas_C_O))
WE_CO_nuevo=WE_CO.copy()
#Iteración funciona como un contador, para determinar la posición en H
iteracion=0
#Con dos bucles for se hace las combinaciones de los dos pesos
for a in range(len(W1_1)):
  for b in range(len(W1_2)):
    WE_CO_nuevo[0,0]=W1_1[a]
    WE_CO_nuevo[1,0]=W1_2[b]
    Contador=np.random.permutation(len(y))
    for i in range(len(Contador)):
      for h in range(Neuronas_C_O):
        valor=np.dot(G[Contador[i],:],WE_CO_nuevo[:,h])
        SCO[h]= 1/(1+math.exp(-valor))
      valor2= np.dot(SCO,WCO_S)
      SALIDA[Contador[i]]= 1/(1+math.exp(-valor2))
    c=(np.transpose(y)-SALIDA)**2
    d=sum(c.reshape(-1,1))
    E2=(1/2)*sum(d)

    y_binario = [1 if val > 0.5 else 0 for val in SALIDA]
    Accuracy=np.mean(np.array(y_binario) == np.array(y))

    H[iteracion,0]=W1_1[a]
    H[iteracion,1]=W1_2[b]
    H[iteracion,2]=Accuracy
    iteracion=iteracion+1

import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm
from matplotlib.ticker import MaxNLocator



fig = plt.figure(figsize=(14,6))
ax = fig.add_subplot(111, projection='3d')

surf=ax.plot_trisurf(H[:,0],H[:,1],H[:,2], linewidth=0, antialiased=False,cmap=cm.jet)
fig.colorbar(surf)

ax.view_init(20,125)
ax.set_title('Accuracy vs pesos sinápticos',fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})
ax.set_xlabel("W11", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})
ax.set_ylabel("W12", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})
ax.set_zlabel("E2", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})
fig.tight_layout()
plt.show()

#Se aplica sumulated annealing

Cantidad_Entradas=35
Neuronas_C_O=20
Neuronas_C_S=1
G=X
#vector que guarda la salida de la neurona de la capa oculta
SCO = np.zeros(Neuronas_C_O)
# Salida de la red
SALIDA= np.zeros(len(y))
#--------------------------------------------------------------------------------------------------------
#se genera el punto inicial dentro de los limites definidos
limite_superior=15
limite_inferior=-15
#ENTRADA-CAPA OCULTA
WE_CO=limite_inferior+(limite_superior-limite_inferior)*np.random.rand(Cantidad_Entradas,Neuronas_C_O)
#CAPA OCULTA-SALIDA
WCO_S=limite_inferior+(limite_superior-limite_inferior)*np.random.rand(Neuronas_C_O)
#---------------------------------------------------------------------------------------------------------
#Se define la función objetivo
def Modelo_machine_learning(A,B):
  Contador=np.random.permutation(len(y))
  for i in range(len(Contador)):
    for h in range(Neuronas_C_O):
      valor=np.dot(G[Contador[i],:],A[:,h])
      SCO[h]= 1/(1+math.exp(-valor))
    valor2= np.dot(SCO,B)
    SALIDA[Contador[i]]= 1/(1+math.exp(-valor2))
  a=(np.transpose(y)-SALIDA)**2
  b=sum(a.reshape(-1,1))
  ECUAD=(1/2)*sum(b)
  return SALIDA,ECUAD
#---------------------------------------------------------------------------------------------------------
#se evalua el punto inicial
Y_actual,ECUAD_actual= Modelo_machine_learning(WE_CO,WCO_S)
# se guarda la solución actual
WE_CO_actual=WE_CO.copy()
WCO_S_actual=WCO_S.copy()
Y_actual=Y_actual.copy()
#-----------------------------------------------------------------------------------------------------------
#se comienza las iteraciones
iteraciones=30000
tamaño_paso=0.05
#se deefine la temperatura inicial
t_inicial=1000
ECUAD_modelo= np.zeros(iteraciones)
for i in range(iteraciones):
#se avanza un paso
  WE_CO_candidato=WE_CO_actual +  np.random.randn(Cantidad_Entradas,Neuronas_C_O)*tamaño_paso
  WCO_S_candidato=WCO_S_actual + np.random.randn(Neuronas_C_O)*tamaño_paso
#se evalua el nuevo paso
  Y_candidato,ECUAD_candidato= Modelo_machine_learning(WE_CO_candidato,WCO_S_candidato)
#se compara la solución actual con la candidata
  Diferencia= ECUAD_candidato-ECUAD_actual
#se calcula la temperatura, se divide por la iteración para que tienda a 0 a meedida que aumente las iteraciones
  t=t_inicial/(i+1)
#se calcular el criterio de aceptación
  criterio=math.exp(-(Diferencia/t))
#se acepta la mejor solución, con una probabilidad de aceptar una solución peor
  ñ=abs(np.random.randn())
  if Diferencia < 0 or ñ< criterio:
    WE_CO_actual=WE_CO_candidato
    WCO_S_actual=WCO_S_candidato
    ECUAD_actual=ECUAD_candidato
  ECUAD_modelo[i]=ECUAD_actual

fig, ax = plt.subplots()
ax.plot(ECUAD_modelo)
ax.set_xlabel("iteraciones", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})
ax.set_ylabel("Error cuadrático", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})
plt.show()

#Discriminador de dígito
#se re adapta y

y_digito = np.eye(10)
print(y_digito)

#Arquitectura de la red
Cantidad_Entradas=35
Neuronas_C_O=4
Neuronas_C_S=10
#Inicialización de los pesos sinápticos
np.random.seed(1)
#ENTRADA-CAPA OCULTA
WE_CO=-1+2*np.random.rand(Cantidad_Entradas,Neuronas_C_O)
#CAPA OCULTA-SALIDA
WCO_S=-1+2*np.random.rand(Neuronas_C_O,Neuronas_C_S)
iter=100
Aprendizaje=0.25
#se crea una matriz que contenga todas las entradas para que luego sea mas facil acceder a esa información
G=X

#vector que guarda la salida de la neurona de la capa oculta
SCO = np.zeros(Neuronas_C_O)
# Salida de la red y error cuadratico
ECUAD= np.zeros(iter)
SALIDA= np.zeros((len(G), Neuronas_C_S)) #la salida es 10 x 10
#comienzo del modelo
for t in range(iter):
# Al añadir este bucle permite que el orden de entrenamiento(patron empleado) sea aleatorio
  Contador=np.random.permutation(len(G))
  for i in range(len(Contador)):
# FEEDFORWARD
#para cada neurona de la capa oculta se calcula su salida multiplicando el peso correspondiente por la entrada, realizando la suma
#y la función de transferencia
    for h in range(Neuronas_C_O):
      # suma de los pesos por la entrada
      valor=np.dot(G[Contador[i],:],WE_CO[:,h])
      #se aplica la función de transferencia
      SCO[h]= 1/(1+math.exp(-valor))
    # Se calcula la salida de la red de forma similar que para las neuronas de la capa interna
    valor2= np.dot(SCO,WCO_S)
    SALIDA[Contador[i],:]= 1/(1+np.exp(-valor2))
# BACKPROPAGATION
# ERROR SALIDA,OCULTA
    delta=(y_digito[Contador[i]]-SALIDA[Contador[i],:])*(1-SALIDA[Contador[i],:])*SALIDA[Contador[i],:]
# se crea una copia de WCO_S para utilizarlo en la siguiente sección
    WCO_SOLD=WCO_S.copy()
    WCO_S+= Aprendizaje*np.outer(SCO, delta)

#ERROR OCULTA,ENTRADA
    deltaOE=SCO *(1-SCO)*(delta @ WCO_SOLD.T)
# A es un vector intermedio
    for j in range(len(deltaOE)):
      WE_CO[:,j]+= Aprendizaje*deltaOE[j] * G[Contador[i],:]
# Se calcula el error cuadratico
  a = (y_digito - SALIDA) ** 2
  b = np.sum(a, axis=0)
  ECUAD[t] = (1 / 2) * np.sum(b)

fig, ax = plt.subplots()
ax.plot(ECUAD)
ax.set_xlabel("iteraciones", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})
ax.set_ylabel("Error cuadrático", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})
plt.ylim(0)
plt.show()


print(SALIDA)

#se añade bias
y_digito = np.eye(10)
print(y_digito)

#Arquitectura de la red
Cantidad_Entradas=35
Neuronas_C_O=4
Neuronas_C_S=10
#Inicialización de los pesos sinápticos
np.random.seed(1)
#ENTRADA-CAPA OCULTA
WE_CO=-1+2*np.random.rand(Cantidad_Entradas,Neuronas_C_O)
#CAPA OCULTA-SALIDA
WCO_S=-1+2*np.random.rand(Neuronas_C_O,Neuronas_C_S)
iter=1000
Aprendizaje=0.25
#se añade el bias

b_CO = np.zeros(Neuronas_C_O)  # Bias para la capa oculta
b_S = np.zeros(Neuronas_C_S)    # Bias para la capa de salida

#se crea una matriz que contenga todas las entradas para que luego sea mas facil acceder a esa información
G=X


#vector que guarda la salida de la neurona de la capa oculta
SCO = np.zeros(Neuronas_C_O)
# Salida de la red y error cuadratico
ECUAD= np.zeros(iter)
SALIDA= np.zeros((len(G), Neuronas_C_S)) #la salida es 10 x 10
#comienzo del modelo
for t in range(iter):
# Al añadir este bucle permite que el orden de entrenamiento(patron empleado) sea aleatorio
  Contador=np.random.permutation(len(G))
  for i in range(len(Contador)):
# FEEDFORWARD
#para cada neurona de la capa oculta se calcula su salida multiplicando el peso correspondiente por la entrada, realizando la suma
#y la función de transferencia
    for h in range(Neuronas_C_O):
      # suma de los pesos por la entrada
      valor=np.dot(G[Contador[i],:],WE_CO[:,h]) +  b_CO[h]
      #se aplica la función de transferencia
      SCO[h]= 1/(1+math.exp(-valor))
    # Se calcula la salida de la red de forma similar que para las neuronas de la capa interna
    valor2= np.dot(SCO,WCO_S)+ b_S
    SALIDA[Contador[i],:]= 1/(1+np.exp(-valor2))
# BACKPROPAGATION
# ERROR SALIDA,OCULTA
    delta=(y_digito[Contador[i]]-SALIDA[Contador[i],:])*(1-SALIDA[Contador[i],:])*SALIDA[Contador[i],:]
# se crea una copia de WCO_S para utilizarlo en la siguiente sección
    WCO_SOLD=WCO_S.copy()
    WCO_S+= Aprendizaje*np.outer(SCO, delta)

#ERROR OCULTA,ENTRADA
    deltaOE=SCO *(1-SCO)*(delta @ WCO_SOLD.T)

    for j in range(len(deltaOE)):
      WE_CO[:,j]+= Aprendizaje*deltaOE[j] * G[Contador[i],:]

#se actualiza el bias

  b_S += Aprendizaje * delta
  b_CO += Aprendizaje * deltaOE

# Se calcula el error cuadratico
  a = (y_digito - SALIDA) ** 2
  b = np.sum(a, axis=0)
  ECUAD[t] = (1 / 2) * np.sum(b)

fig, ax = plt.subplots()
ax.plot(ECUAD)
ax.set_xlabel("iteraciones", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})
ax.set_ylabel("Error cuadrático", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})
plt.ylim(0)
plt.show()


print(SALIDA)

#Discriminador de dígito
#se readapta y
#se compara optimizadores

y_digito = np.eye(10)
#Arquitectura de la red
Cantidad_Entradas=35
Neuronas_C_O=20
Neuronas_C_S=10
#Inicialización de los pesos sinápticos
np.random.seed(1)
#ENTRADA-CAPA OCULTA
WE_CO=-1+2*np.random.rand(Cantidad_Entradas,Neuronas_C_O)
#CAPA OCULTA-SALIDA
WCO_S=-1+2*np.random.rand(Neuronas_C_O,Neuronas_C_S)
iter=250
Aprendizaje=0.25
#se crea una matriz que contenga todas las entradas para que luego sea mas facil acceder a esa información
G=X


#vector que guarda la salida de la neurona de la capa oculta
SCO = np.zeros(Neuronas_C_O)
# Salida de la red y error cuadratico
ECUAD_grad= np.zeros(iter)
SALIDA= np.zeros((len(G), Neuronas_C_S)) #la salida es 10 x 10
#comienzo del modelo
for t in range(iter):
# Al añadir este bucle permite que el orden de entrenamiento(patron empleado) sea aleatorio
  Contador=np.random.permutation(len(G))
  for i in range(len(Contador)):
# FEEDFORWARD
#para cada neurona de la capa oculta se calcula su salida multiplicando el peso correspondiente por la entrada, realizando la suma
#y la función de transferencia
    for h in range(Neuronas_C_O):
      # suma de los pesos por la entrada
      valor=np.dot(G[Contador[i],:],WE_CO[:,h])
      #se aplica la función de transferencia
      SCO[h]= 1/(1+math.exp(-valor))
    # Se calcula la salida de la red de forma similar que para las neuronas de la capa interna
    valor2= np.dot(SCO,WCO_S)
    SALIDA[Contador[i],:]= 1/(1+np.exp(-valor2))
# BACKPROPAGATION
# ERROR SALIDA,OCULTA
    delta=(y_digito[Contador[i]]-SALIDA[Contador[i],:])*(1-SALIDA[Contador[i],:])*SALIDA[Contador[i],:]
# se crea una copia de WCO_S para utilizarlo en la siguiente sección
    WCO_SOLD=WCO_S.copy()
    WCO_S+= Aprendizaje*np.outer(SCO, delta)

#ERROR OCULTA,ENTRADA
    deltaOE=SCO *(1-SCO)*(delta @ WCO_SOLD.T)
# A es un vector intermedio
    for j in range(len(deltaOE)):
      WE_CO[:,j]+= Aprendizaje*deltaOE[j] * G[Contador[i],:]
# Se calcula el error cuadratico
  a = (y_digito - SALIDA) ** 2
  b = np.sum(a, axis=0)
  ECUAD_grad[t] = (1 / 2) * np.sum(b)

#------------------------------------------------------------------------------------
#Inicialización de los pesos sinápticos
np.random.seed(1)
#ENTRADA-CAPA OCULTA
WE_CO=-1+2*np.random.rand(Cantidad_Entradas,Neuronas_C_O)
#CAPA OCULTA-SALIDA
WCO_S=-1+2*np.random.rand(Neuronas_C_O,Neuronas_C_S)
Aprendizaje=0.25
#se crea una matriz que contenga todas las entradas para que luego sea mas facil acceder a esa información
G=X
# Inicialización de las velocidades
v_WE_CO = np.zeros_like(WE_CO)
v_WCO_S = np.zeros_like(WCO_S)
momentum = 0.7


#vector que guarda la salida de la neurona de la capa oculta
SCO = np.zeros(Neuronas_C_O)
# Salida de la red y error cuadratico
ECUAD_momen= np.zeros(iter)
SALIDA= np.zeros((len(G), Neuronas_C_S)) #la salida es 10 x 10
#comienzo del modelo
for t in range(iter):
# Al añadir este bucle permite que el orden de entrenamiento(patron empleado) sea aleatorio
  Contador=np.random.permutation(len(G))
  for i in range(len(Contador)):
# FEEDFORWARD
#para cada neurona de la capa oculta se calcula su salida multiplicando el peso correspondiente por la entrada, realizando la suma
#y la función de transferencia
    for h in range(Neuronas_C_O):
      # suma de los pesos por la entrada
      valor=np.dot(G[Contador[i],:],WE_CO[:,h])
      #se aplica la función de transferencia
      SCO[h]= 1/(1+math.exp(-valor))
    # Se calcula la salida de la red de forma similar que para las neuronas de la capa interna
    valor2= np.dot(SCO,WCO_S)
    SALIDA[Contador[i],:]= 1/(1+np.exp(-valor2))
# BACKPROPAGATION
# ERROR SALIDA,OCULTA
    delta=(y_digito[Contador[i]]-SALIDA[Contador[i],:])*(1-SALIDA[Contador[i],:])*SALIDA[Contador[i],:]

    WCO_SOLD=WCO_S.copy()

    v_WCO_S = momentum * v_WCO_S + Aprendizaje * np.outer(SCO, delta)

    WCO_S+= v_WCO_S

#ERROR OCULTA,ENTRADA
    deltaOE=SCO *(1-SCO)*(delta @ WCO_SOLD.T)
# A es un vector intermedio
    for j in range(len(deltaOE)):
      v_WE_CO[:, j] = momentum * v_WE_CO[:, j] + Aprendizaje * deltaOE[j] * G[Contador[i], :]
      WE_CO[:, j] += v_WE_CO[:, j]

# Se calcula el error cuadratico
  a = (y_digito - SALIDA) ** 2
  b = np.sum(a, axis=0)
  ECUAD_momen[t] = (1 / 2) * np.sum(b)

#----------------------------------------------------------------------------

#Inicialización de los pesos sinápticos
np.random.seed(1)
#ENTRADA-CAPA OCULTA
WE_CO=-1+2*np.random.rand(Cantidad_Entradas,Neuronas_C_O)
#CAPA OCULTA-SALIDA
WCO_S=-1+2*np.random.rand(Neuronas_C_O,Neuronas_C_S)
Aprendizaje=0.25
#se crea una matriz que contenga todas las entradas para que luego sea mas facil acceder a esa información
G=X
#parametros del optimizador
beta1 = 0.7
beta2 = 0.8
epsilon = 1e-8

# Inicialización de las velocidades
m_WE_CO = np.zeros_like(WE_CO)
m_WCO_S = np.zeros_like(WCO_S)
v_WE_CO = np.zeros_like(WE_CO)
v_WCO_S = np.zeros_like(WCO_S)


#vector que guarda la salida de la neurona de la capa oculta
SCO = np.zeros(Neuronas_C_O)
# Salida de la red y error cuadratico
ECUAD_adam= np.zeros(iter)
SALIDA= np.zeros((len(G), Neuronas_C_S)) #la salida es 10 x 10
#comienzo del modelo
for t in range(iter):
# Al añadir este bucle permite que el orden de entrenamiento(patron empleado) sea aleatorio
  Contador=np.random.permutation(len(G))
  for i in range(len(Contador)):
# FEEDFORWARD
#para cada neurona de la capa oculta se calcula su salida multiplicando el peso correspondiente por la entrada, realizando la suma
#y la función de transferencia
    for h in range(Neuronas_C_O):
      # suma de los pesos por la entrada
      valor=np.dot(G[Contador[i],:],WE_CO[:,h])
      #se aplica la función de transferencia
      SCO[h]= 1/(1+math.exp(-valor))
    # Se calcula la salida de la red de forma similar que para las neuronas de la capa interna
    valor2= np.dot(SCO,WCO_S)
    SALIDA[Contador[i],:]= 1/(1+np.exp(-valor2))
# BACKPROPAGATION
# ERROR SALIDA,OCULTA
    delta=(y_digito[Contador[i]]-SALIDA[Contador[i],:])*(1-SALIDA[Contador[i],:])*SALIDA[Contador[i],:]
#se actualiza m y v

    m_WCO_S = beta1 * m_WCO_S + (1 - beta1) * np.outer(SCO, delta)
    v_WCO_S = beta2 * v_WCO_S + (1 - beta2) * np.outer(SCO, delta) ** 2

# Correción de sesgo
    m_WCO_S_hat = m_WCO_S / (1 - beta1 ** (t + 1))
    v_WCO_S_hat = v_WCO_S / (1 - beta2 ** (t + 1))


    WCO_SOLD=WCO_S.copy()

    WCO_S += Aprendizaje * m_WCO_S_hat / (np.sqrt(v_WCO_S_hat) + epsilon)

#ERROR OCULTA,ENTRADA
    deltaOE=SCO *(1-SCO)*(delta @ WCO_SOLD.T)

    for j in range(len(deltaOE)):

      m_WE_CO[:, j] = beta1 * m_WE_CO[:, j] + (1 - beta1) * deltaOE[j] * G[Contador[i], :]
      v_WE_CO[:, j] = beta2 * v_WE_CO[:, j] + (1 - beta2) * (deltaOE[j] * G[Contador[i], :]) ** 2

      # Corrección de sesgo
      m_WE_CO_hat = m_WE_CO / (1 - beta1 ** (t + 1))
      v_WE_CO_hat = v_WE_CO / (1 - beta2 ** (t + 1))

      WE_CO[:, j] += Aprendizaje * m_WE_CO_hat[:, j] / (np.sqrt(v_WE_CO_hat[:, j]) + epsilon)



# Se calcula el error cuadratico
  a = (y_digito - SALIDA) ** 2
  b = np.sum(a, axis=0)
  ECUAD_adam[t] = (1 / 2) * np.sum(b)



fig, ax = plt.subplots()
ax.plot(ECUAD_grad,label='Gradiente')
ax.plot(ECUAD_momen,label='Momentum')
ax.plot(ECUAD_adam,label='Adam')
ax.set_xlabel("iteraciones", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})
ax.set_ylabel("Error cuadrático", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})
plt.ylim(0,10)
ax.legend()
plt.show()


print(SALIDA)

#se corre le modelo para ajustarse a los datos iniciales
#se entrena el modelo
#se añade bias
y_digito = np.eye(10)
#Arquitectura de la red
Cantidad_Entradas=35
Neuronas_C_O=20
Neuronas_C_S=10
#Inicialización de los pesos sinápticos
np.random.seed(1)
#ENTRADA-CAPA OCULTA
WE_CO=-1+2*np.random.rand(Cantidad_Entradas,Neuronas_C_O)
#CAPA OCULTA-SALIDA
WCO_S=-1+2*np.random.rand(Neuronas_C_O,Neuronas_C_S)
iter=500
Aprendizaje=0.25
#se añade el bias

b_CO = np.zeros(Neuronas_C_O)  # Bias para la capa oculta
b_S = np.zeros(Neuronas_C_S)    # Bias para la capa de salida

#se crea una matriz que contenga todas las entradas para que luego sea mas facil acceder a esa información
G=X


#vector que guarda la salida de la neurona de la capa oculta
SCO = np.zeros(Neuronas_C_O)
# Salida de la red y error cuadratico
ECUAD= np.zeros(iter)
SALIDA= np.zeros((len(G), Neuronas_C_S)) #la salida es 10 x 10
#comienzo del modelo
for t in range(iter):
# Al añadir este bucle permite que el orden de entrenamiento(patron empleado) sea aleatorio
  Contador=np.random.permutation(len(G))
  for i in range(len(Contador)):
# FEEDFORWARD
#para cada neurona de la capa oculta se calcula su salida multiplicando el peso correspondiente por la entrada, realizando la suma
#y la función de transferencia
    for h in range(Neuronas_C_O):
      # suma de los pesos por la entrada
      valor=np.dot(G[Contador[i],:],WE_CO[:,h]) +  b_CO[h]
      #se aplica la función de transferencia
      SCO[h]= 1/(1+math.exp(-valor))
    # Se calcula la salida de la red de forma similar que para las neuronas de la capa interna
    valor2= np.dot(SCO,WCO_S)+ b_S
    SALIDA[Contador[i],:]= 1/(1+np.exp(-valor2))
# BACKPROPAGATION
# ERROR SALIDA,OCULTA
    delta=(y_digito[Contador[i]]-SALIDA[Contador[i],:])*(1-SALIDA[Contador[i],:])*SALIDA[Contador[i],:]
# se crea una copia de WCO_S para utilizarlo en la siguiente sección
    WCO_SOLD=WCO_S.copy()
    WCO_S+= Aprendizaje*np.outer(SCO, delta)

#ERROR OCULTA,ENTRADA
    deltaOE=SCO *(1-SCO)*(delta @ WCO_SOLD.T)
    for j in range(len(deltaOE)):
      WE_CO[:,j]+= Aprendizaje*deltaOE[j] * G[Contador[i],:]

#se actualiza el bias

  b_S += Aprendizaje * delta
  b_CO += Aprendizaje * deltaOE

# Se calcula el error cuadratico
  a = (y_digito - SALIDA) ** 2
  b = np.sum(a, axis=0)
  ECUAD[t] = (1 / 2) * np.sum(b)

fig, ax = plt.subplots()
ax.plot(ECUAD)
ax.set_xlabel("iteraciones", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})
ax.set_ylabel("Error cuadrático", fontdict = {'fontsize':14, 'fontweight':'bold', 'color':'tab:blue'})
plt.ylim(0)
plt.show()

Contador=np.random.permutation(len(G))
for i in range(len(Contador)):
  for h in range(Neuronas_C_O):
    valor=np.dot(G[Contador[i],:],WE_CO[:,h]) +  b_CO[h]
    SCO[h]= 1/(1+math.exp(-valor))
    valor2= np.dot(SCO,WCO_S)+ b_S
    SALIDA[Contador[i],:]= 1/(1+np.exp(-valor2))

a = (y_digito - SALIDA) ** 2
b = np.sum(a, axis=0)
error_cuadratico= (1 / 2) * np.sum(b)

y_prediccion=np.zeros_like(SALIDA)

for i in range(SALIDA.shape[0]):
    y_prediccion[i, np.argmax(SALIDA[i])] = 1

print(y_prediccion)

y_pred_etiqueta = []

for i in range(y_prediccion.shape[0]):
    y_pred_etiqueta.append(np.argmax(y_prediccion[i]))

y_pred_etiqueta = np.array(y_pred_etiqueta)

print(y_pred_etiqueta)

y_real_etiqueta = []

for i in range(y_digito.shape[0]):
    y_real_etiqueta.append(np.argmax(y_digito[i]))

y_real_etiqueta = np.array(y_real_etiqueta)

print(y_real_etiqueta)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import seaborn as sns
m = confusion_matrix(y_real_etiqueta,y_pred_etiqueta)

print(m)

#copio el primer patron
G_replicada = np.tile(G[0, :], (10, 1))

for i in range(G_replicada.shape[0]):
    np.random.seed(i)
    Contador2=np.random.permutation(35)
    for j in range(30):
      G_replicada[i,Contador2[j]] = 1 - G_replicada[i,Contador2[j]]



Contador=np.random.permutation(len(G_replicada))
for i in range(len(Contador)):
  for h in range(Neuronas_C_O):
    valor=np.dot(G_replicada[Contador[i],:],WE_CO[:,h]) +  b_CO[h]
    SCO[h]= 1/(1+math.exp(-valor))
    valor2= np.dot(SCO,WCO_S)+ b_S
    SALIDA[Contador[i],:]= 1/(1+np.exp(-valor2))

a = (y_digito - SALIDA) ** 2
b = np.sum(a, axis=0)
error_cuadratico= (1 / 2) * np.sum(b)

y_prediccion=np.zeros_like(SALIDA)

for i in range(SALIDA.shape[0]):
    y_prediccion[i, np.argmax(SALIDA[i])] = 1



y_pred_etiqueta = []

for i in range(y_prediccion.shape[0]):
    y_pred_etiqueta.append(np.argmax(y_prediccion[i]))

y_pred_etiqueta = np.array(y_pred_etiqueta)

print(y_pred_etiqueta)

SALIDA